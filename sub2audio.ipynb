{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main use case:\n",
    "\n",
    "## You have video with non-English audio, but you have English subtitles (or going to prepare). Now you are ready to generate new English audio-track for this video.\n",
    "\n",
    "\n",
    "This notebook working steps:\n",
    "\n",
    "1) Download captions for video id, save forever to local file, so, if captions changed, delete local pickle-file and repeat.\n",
    "2) Parse and clean text from captions\n",
    "3) Split text to sentences and synthesize WAV files for each sentence by **Mozilla TTS**.\n",
    "4) Arrange start point of each audio by matching text with subtitles. Visualize subtitles before and after arrangement by heat-map image. Rows = minutes. Cols = seconds. Numbers in cells = numbers of audio segment.\n",
    "5) Concatenate all audio segment (and background music if not disabled) in one wav audio track.\n",
    "6) *Replace audio track in local video file.* Deprecated: you should replace audio-track in any video editor manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "youtube_video_id = 'ImdWoHviA0k'\n",
    "\n",
    "# folder where additional folders will be created for every youtube_video_id\n",
    "data_folder_root = Path('s:/temp/data')\n",
    "# data_folder_root = Path().absolute()/'caps2audio'  # uncomment this to use current folder\n",
    "data_folder = data_folder_root/youtube_video_id\n",
    "\n",
    "# set your path to .json after installing Coqui-AI TTS by pip\n",
    "tss_models_json_path = \"C:/Python/Python39/Lib/site-packages/TTS/.models.json\"\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "os.makedirs(data_folder/'wavs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: complete list of required modules to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "%pip install youtube-transcript-api nltk TTS soundfile pydub webvtt-py librosa seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "PICKLED_CAPTIONS = data_folder/'subtitles.pickle'\n",
    "try:\n",
    "    with open(PICKLED_CAPTIONS, 'rb') as f:\n",
    "        temp_captions = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    temp_captions = YouTubeTranscriptApi.get_transcript(youtube_video_id, languages=['en'])\n",
    "    print('Loading from PICKLE failed. Downloading from Youtube...')\n",
    "    with open(PICKLED_CAPTIONS, 'wb') as f:\n",
    "        pickle.dump(temp_captions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Caption:\n",
    "    text: str\n",
    "    start: float\n",
    "    duration: float\n",
    "    def __repr__(self):\n",
    "            return f\"{self.timestamp_start()} --> {self.timestamp_end()}\\n{self.text}\"\n",
    "    def __post_init__(self):\n",
    "            # self.end = self.start + self.duration\n",
    "            object.__setattr__(self, 'end', self.start + self.duration)  # syntax for case @dataclass(frozen=True)\n",
    "    def timestamp_start(self) -> str: \n",
    "        \"\"\" Convert seconds to HH:MM:SS,mmm format \"\"\"\n",
    "        hours = int(self.start / 3600)\n",
    "        minutes = int(self.start / 60 - hours * 60)\n",
    "        seconds = self.start - hours * 3600 - minutes * 60\n",
    "        return '{:02d}:{:02d}:{:06.3f}'.format(hours, minutes, seconds)\n",
    "    def timestamp_end(self) -> str:\n",
    "        \"\"\" Convert seconds to HH:MM:SS,mmm format \"\"\"\n",
    "        end = self.start + self.duration\n",
    "        hours = int(end / 3600)\n",
    "        minutes = int(end / 60 - hours * 60)\n",
    "        seconds = end - hours * 3600 - minutes * 60\n",
    "        return '{:02d}:{:02d}:{:06.3f}'.format(hours, minutes, seconds)\n",
    "\n",
    "captions: List[Caption] = []\n",
    "for caption in temp_captions:\n",
    "    captions.append(Caption(text=caption['text'], start=caption['start'], duration=caption['duration']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional cell. Save subtitles on disk in WEBTT and SRT formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "\n",
    "vtt = webvtt.WebVTT()\n",
    "for cap in captions:\n",
    "    vtt.captions.append(webvtt.Caption(cap.timestamp_start(), cap.timestamp_end(), cap.text))\n",
    "vtt.save(str(data_folder/youtube_video_id) + '.vtt')\n",
    "vtt.save_as_srt(str(data_folder/youtube_video_id) + '.srt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare text before processing by NLTK library. Better to have simple sentences and no special chars inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "full_text = ' '.join([cap.text for cap in captions])\n",
    "full_text = full_text.replace('  ', ' ')\n",
    "full_text = re.sub(r'([A-Za-z]) +(\\.)', r\"\\1\\2\", full_text)\n",
    "full_text = re.sub(r'\\.+', '.', full_text)\n",
    "full_text = re.sub(r'\\. +\\.', '.', full_text)\n",
    "full_text = full_text.replace('â€™', \"'\")\n",
    "with open(data_folder/'full_text.txt', 'w') as f:  # optional\n",
    "    f.write(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last chance to make corrections in text or to rewrite.\n",
    "You can edit text in edited_full_text.txt and add cell to load changes: \n",
    "\n",
    "`\n",
    "full_text = open(data_folder/'edited_full_text.txt', 'r').read()\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_text = open(data_folder/'edited_full_text.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences: List[str] = sent_tokenize(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new_captions: List[NewCaption] from these sentences.\n",
    "\n",
    "Initial start point and duration is zero (will be set later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class NewCaption(Caption):\n",
    "    text: str\n",
    "    start: float = 0\n",
    "    duration: float = 0\n",
    "    wav_path: str = ''\n",
    "    wav_duration: float = 0\n",
    "    def meta_info(self) -> str:\n",
    "        res = ''\n",
    "        if self.wav_path:\n",
    "            res = os.path.split(self.wav_path)[1].split('.')[0] + '|' + self.text\n",
    "        return res\n",
    "\n",
    "new_captions: List[NewCaption] = []\n",
    "for sent in sentences:\n",
    "    new_captions.append(NewCaption(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate WAV files for each sentence in new_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from TTS.utils.manage import ModelManager\n",
    "from TTS.utils.synthesizer import Synthesizer\n",
    "from tqdm.auto import tqdm, trange\n",
    "import librosa\n",
    "\n",
    "manager = ModelManager(tss_models_json_path)\n",
    "# model_name = \"tts_models/en/ljspeech/tacotron2-DCA\"\n",
    "model_name = \"tts_models/en/ljspeech/tacotron2-DDC_ph\"\n",
    "# vocoder_name = \"vocoder_models/en/ljspeech/multiband-melgan\"\n",
    "vocoder_name = \"vocoder_models/en/ljspeech/univnet\"\n",
    "model_path, config_path, model_item = manager.download_model(model_name)\n",
    "vocoder_path, vocoder_config_path, _ = manager.download_model(vocoder_name)\n",
    "synthesizer = Synthesizer(\n",
    "    model_path,\n",
    "    config_path,\n",
    "    None,\n",
    "    None,\n",
    "    vocoder_path,\n",
    "    vocoder_config_path,\n",
    "    None,\n",
    "    None,\n",
    "    USE_CUDA,\n",
    ")\n",
    "\n",
    "os.makedirs(data_folder/'wavs', exist_ok=True)\n",
    "\n",
    "resynthesize_existed = False  # for debug purposes\n",
    "\n",
    "cur = 0\n",
    "for i, cap in enumerate(tqdm(new_captions)):\n",
    "    cap.wav_path = str(data_folder/'wavs'/f'{i + 1:03}.wav')\n",
    "    if resynthesize_existed or not os.path.isfile(cap.wav_path):\n",
    "        wav = synthesizer.tts(new_captions[i].text, None, None, None, reference_wav=None, reference_speaker_name=None,)\n",
    "    else:\n",
    "        wav, _ = librosa.load(cap.wav_path)\n",
    "    cap.wav_duration = librosa.get_duration(wav)\n",
    "    cap.duration = cap.wav_duration\n",
    "    cap.start = cur\n",
    "    cur = cur + cap.wav_duration\n",
    "    synthesizer.save_wav(wav, new_captions[i].wav_path)\n",
    "    print(f'{cap=}\\n')\n",
    "\n",
    "with open(data_folder/'metadata.csv', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join([cap.meta_info() for cap in new_captions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional. Generate audios (in folder 'wavs-alt') with another model to compare duration of each file to find bad-generated by DCA model with 'attentions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# model_name = \"tts_models/en/sam/tacotron-DDC\"\n",
    "# vocoder_name = \"vocoder_models/en/sam/hifigan_v2\"\n",
    "model_name = \"tts_models/en/ljspeech/tacotron2-DCA\"\n",
    "vocoder_name = \"vocoder_models/en/ljspeech/multiband-melgan\"\n",
    "\n",
    "model_path, config_path, model_item = manager.download_model(model_name)\n",
    "vocoder_path, vocoder_config_path, _ = manager.download_model(vocoder_name)\n",
    "synthesizer = Synthesizer(\n",
    "    model_path,\n",
    "    config_path,\n",
    "    None,\n",
    "    None,\n",
    "    vocoder_path,\n",
    "    vocoder_config_path,\n",
    "    None,\n",
    "    None,\n",
    "    True,\n",
    ")\n",
    "os.makedirs(data_folder/'wavs-alt', exist_ok=True)\n",
    "for i in range(0, len(new_captions)):\n",
    "    wav = synthesizer.tts(\n",
    "        new_captions[i].text,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        reference_wav=None,\n",
    "        reference_speaker_name=None,\n",
    "    )\n",
    "    synthesizer.save_wav(wav, new_captions[i].wav_path.replace('wavs', 'wavs-alt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional. Compare wavs and wavs-alt by duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from tqdm.auto import trange\n",
    "\n",
    "for i in trange(0, len(new_captions)):\n",
    "    wav_alt = str(new_captions[i].wav_path).replace('wavs', 'wavs-alt')\n",
    "    sound_file_alt = sf.SoundFile(wav_alt)\n",
    "    duration_alt = len(sound_file_alt) / sound_file_alt.samplerate\n",
    "    ratio = new_captions[i].wav_duration / duration_alt\n",
    "    filler = ' '\n",
    "    if ratio < 0.97:\n",
    "        print(f'{i + 1:03}.wav {new_captions[i].wav_duration:{filler}>7.3f}  ~~>{duration_alt:{filler}>7.3f}         Ratio:{ratio:{filler}>7.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional. Process audio files by DeepSpeech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy deepspeech deepspeech-gpu\n",
    "\n",
    "import wave\n",
    "from deepspeech import Model \n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "ds_model = 'S:/temp/data/models/deepspeech-0.9.3-models.pbmm'\n",
    "ds = Model(ds_model)\n",
    "ds_scorer = 'S:/temp/data/models/deepspeech-0.9.3-models.scorer'\n",
    "ds.enableExternalScorer(ds_scorer)\n",
    "\n",
    "recognized_texts: List[str] = []\n",
    "for cap in tqdm(new_captions):\n",
    "    y, sr = librosa.load(cap.wav_path, sr=16000)\n",
    "    audio = (y * 32767).astype(np.int16)\n",
    "    recognized_texts.append(ds.stt(audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional. Compare recognized texts with original text in subtitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(recognized_texts):\n",
    "    num_words_in_res = len(sent.split())\n",
    "    num_words_in_original = len(new_captions[i].text.split())\n",
    "    ratio = num_words_in_res / num_words_in_original\n",
    "    if num_words_in_original < 10:\n",
    "      continue\n",
    "    if ratio <= 0.85:\n",
    "        print(f'\\n!!! {i + 1:03}.wav ---------------------------------------------------------- ratio = {ratio:3.2f}')\n",
    "        print(sent)\n",
    "        print(new_captions[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional. Visualize beginning state of subtitles. All wavs aligned to left on timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "whole_video_duration_by_captions = int(captions[-1].start + captions[-1].duration) + 1\n",
    "cols = 60\n",
    "rows = whole_video_duration_by_captions // 60 + 1\n",
    "rounder_dur = whole_video_duration_by_captions + (60 - whole_video_duration_by_captions % 60)\n",
    "\n",
    "time_matrix = np.zeros(rounder_dur, dtype=np.int)\n",
    "for i, cap in enumerate(new_captions):\n",
    "    time_matrix[int(cap.start)] = i + 1\n",
    "    for j in range(1, math.ceil(cap.duration)):\n",
    "        time_matrix[int(cap.start) + j] = i + 1\n",
    "time_matrix = time_matrix.reshape(rows, 60)\n",
    "\n",
    "time_matrix_without_zeros = time_matrix.copy()\n",
    "time_matrix_without_zeros = time_matrix_without_zeros.astype(str)\n",
    "time_matrix_without_zeros[time_matrix_without_zeros == '0'] = ''\n",
    "\n",
    "%matplotlib qt\n",
    "num_fo_captions = len(new_captions)\n",
    "palette = [(0, 0, 0)]\n",
    "colors = [(0.860, 0.371, 0.339), (0.568, 0.860, 0.339), (0.631, 0.400, 0.860)]\n",
    "colors *= num_fo_captions // len(colors) + 1\n",
    "palette.extend(colors)\n",
    "new_cmap = matplotlib.colors.ListedColormap(palette)\n",
    "norm = matplotlib.colors.BoundaryNorm(np.arange(0, num_fo_captions + 1), num_fo_captions)\n",
    "ax = sns.heatmap(time_matrix, linewidth=0, annot=time_matrix_without_zeros, fmt=\"s\", cbar=None, cmap=new_cmap, norm=norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new **arranged_new_captions** list and arranging **start** for each WAV file to correct start point on timeline by matching text with text in CAPTIONS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import string\n",
    "import copy\n",
    "\n",
    "\n",
    "arranged_new_captions = copy.deepcopy(new_captions)\n",
    "whole_video_duration_by_captions = int(captions[-1].start + captions[-1].duration) + 1\n",
    "end_point = whole_video_duration_by_captions\n",
    "indexes: List[int] = []\n",
    "indexes.append(len(captions) - 1)  # first index = last\n",
    "\n",
    "show_debug = False\n",
    "def debug(x):\n",
    "    if show_debug:\n",
    "        print(x)\n",
    "\n",
    "debug(f'Number of sentences in new captions: {len(new_captions)}')\n",
    "debug(f'Number of sentences in original captions: {len(captions)}')\n",
    "# Starts moving from last to first subtitle.\n",
    "# Tries to find correct start time for each by comparing text with original (not reshaped to sentences) subtitles. \n",
    "# If can not find correspondence, makes it 'float' (can be moved by neighbor)\n",
    "for new_cap in reversed(arranged_new_captions): \n",
    "    debug(indexes)\n",
    "    debug(f'\\n\\n{new_cap=}')\n",
    "    congruence_index = len(captions) - 1  # default for case if no correspondent test found\n",
    "    # Squeezes sentence to string only from ascii letters. So search will work more reliable.\n",
    "    # Example: This is a text. --> Thisisatext\n",
    "    sub = ''.join(filter(lambda x: x in string.ascii_letters , new_cap.text))\n",
    "    cap_block = ''  # caption block from captions (squeezed by the same way)\n",
    "    \n",
    "    for i, cap in reversed(list(enumerate(captions))):\n",
    "        # Accumulates captions in cap_block until sentence is found\n",
    "        cap_block = ''.join(filter(lambda x: x in string.ascii_letters , cap.text)) + cap_block\n",
    "        if sub in cap_block:\n",
    "            # Every found subtitle index must be equal or less then previous.\n",
    "            if len(indexes) > 0 and i <= indexes[-1]:\n",
    "                congruence_index = i\n",
    "                break\n",
    "            else:\n",
    "                # If not - remove sub from cap_block to search sub in next subtitles\n",
    "                cap_block = cap_block.replace(sub, '')  # Skip this. We will look in next block left\n",
    "    if congruence_index == len(captions) - 1:\n",
    "        debug('No correspondent block found.')\n",
    "        congruence_index = indexes[-1]\n",
    "    indexes.append(congruence_index)\n",
    "  \n",
    "    start = captions[congruence_index].start\n",
    "    new_cap.start = min(start, (end_point - new_cap.duration)) if new_cap.start < start else new_cap.start\n",
    "    end_point = new_cap.start\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "discrete_difference = np.diff(np.array(indexes))\n",
    "if not all(discrete_difference <= 0):\n",
    "    print('Descending sequence check failed. Something went wrong with indexes.')\n",
    "\n",
    "cols = 60\n",
    "rows = whole_video_duration_by_captions // 60 + 1\n",
    "# Round DURATION in seconds up to the next multiple of 60\n",
    "rounder_dur = whole_video_duration_by_captions + (60 - whole_video_duration_by_captions % 60)\n",
    "\n",
    "time_matrix_2 = np.zeros(rounder_dur,  dtype=np.int)\n",
    "for i, cap in enumerate(arranged_new_captions):\n",
    "    time_matrix_2[int(cap.start)] = i + 1\n",
    "    for j in range(1, math.ceil(cap.duration)):\n",
    "        time_matrix_2[int(cap.start) + j] = i + 1\n",
    "\n",
    "time_matrix_2 = time_matrix_2.reshape(rows, 60)\n",
    "time_matrix_2_without_zeros = time_matrix_2.copy()\n",
    "time_matrix_2_without_zeros = time_matrix_2_without_zeros.astype(str)\n",
    "time_matrix_2_without_zeros[time_matrix_2_without_zeros == '0'] = ''\n",
    "\n",
    "\n",
    "%matplotlib qt\n",
    "num_fo_captions = len(new_captions)\n",
    "palette = [(0, 0, 0)]\n",
    "colors = [(0.860, 0.371, 0.339), (0.568, 0.860, 0.339), (0.631, 0.400, 0.860)]\n",
    "colors *= num_fo_captions // len(colors) + 1\n",
    "palette.extend(colors)\n",
    "new_cmap = matplotlib.colors.ListedColormap(palette)\n",
    "norm = matplotlib.colors.BoundaryNorm(np.arange(0, num_fo_captions + 1), num_fo_captions)\n",
    "ax = sns.heatmap(time_matrix_2, linewidth=0, annot=time_matrix_2_without_zeros, fmt=\"s\", cbar=None, cmap=new_cmap, norm=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate wavs in 'final.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "audio_track = AudioSegment.silent(duration=whole_video_duration_by_captions * 1000)\n",
    "for cap in tqdm(arranged_new_captions):\n",
    "    sound = AudioSegment.from_file(cap.wav_path, format=\"wav\")\n",
    "    audio_track = audio_track.overlay(sound, position=cap.start * 1000)\n",
    "audio_track.export(data_folder/'wavs'/'final.wav', format=\"wav\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Useless caption processing version partially in func-style with map/reduce.\n",
    "\n",
    "# import itertools\n",
    "# import functools\n",
    "# import math\n",
    "# import string\n",
    "# import copy\n",
    "\n",
    "# arranged_new_captions = copy.deepcopy(new_captions)\n",
    "\n",
    "# def shrink(text: str) -> str:\n",
    "#     return ''.join(filter(lambda c: c in string.ascii_letters , text))\n",
    "    \n",
    "# def reduce_search(x, y):\n",
    "#     if x[2] in x[1]:\n",
    "#         if x[0] <= x[3]:\n",
    "#             return x\n",
    "#         else:\n",
    "#             x[1] = x[1].replace(x[2], '')\n",
    "#     return  (y[0], y[1] + x[1], x[2], y[3])\n",
    "\n",
    "# whole_video_duration_by_captions = int(captions[-1].start + captions[-1].duration) + 1\n",
    "# end_point = whole_video_duration_by_captions\n",
    "# indexes: List[int] = []\n",
    "# indexes.append(len(captions) - 1)  # first index = last\n",
    "\n",
    "# captions_for_reduce = list(reversed(list(enumerate(captions))))\n",
    "# captions_for_reduce = list(itertools.starmap(lambda x, cap: (x, shrink(cap.text)), captions_for_reduce))\n",
    "\n",
    "# for new_cap in reversed(arranged_new_captions): \n",
    "#     data = list(itertools.starmap(\n",
    "#         lambda x, captext: (x, captext, shrink(new_cap.text), indexes[-1]),\n",
    "#         captions_for_reduce))\n",
    "#     congruence_index = list(functools.reduce(reduce_search, data))[0]\n",
    "\n",
    "#     if not congruence_index:\n",
    "#         congruence_index = len(captions) - 1\n",
    "#     indexes.append(congruence_index)\n",
    "  \n",
    "#     start = captions[congruence_index].start\n",
    "#     new_cap.start = min(start, (end_point - new_cap.duration)) if new_cap.start < start else new_cap.start\n",
    "#     end_point = new_cap.start"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "25034407fed5d681614dac11a1c0537e8cb49e3a8883c071303eea01322943d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
